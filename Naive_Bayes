## Step.1 Data collection

data_raw <- readLines(con = "SMSSpamCollection.txt")
str(data_raw)
head(data_raw);tail(data_raw)


## Step.2 Data processing and exploration


#split text data
library(tidyverse)
n <- length(data_raw)
type <- c()
text <- c()
data_raw

for(i in 1:n) {
  result <- unlist(str_split(string = data_raw[i], pattern = "\\\t"))
  type[i] <- result[1]
  text[i] <- result[2]
}
data_1 <- data.frame(
  type = type,
  text = text
)

data_1$type <- factor(data_1$type)

str(data_1)
table(data_1$type)

#text corpus
install.packages('tm') #updated
library(tm)

text_corpus <- VectorSource(x = data_1$text) %>% 
  VCorpus()

inspect(head(text_corpus))
text_corpus[[1]] %>% 
  as.character()

#change lower case
text_corpus_clean_1 <- tm_map(x = text_corpus, 
                            FUN = content_transformer(FUN = tolower))

#remove numbers
text_corpus_clean_2 <- tm_map(x = text_corpus_clean_1,
                              FUN = removeNumbers)

#remove specific words
stopwords(kind = "en")
text_corpus_clean_3 <- tm_map(x = text_corpus_clean_2,
                              FUN = removeWords, stopwords())

#remove punctuaion
text_corpus_clean_4 <- tm_map(x = text_corpus_clean_3,
                              FUN = removePunctuation)

#stemming
library(SnowballC)
text_corpus_clean_5 <- tm_map(x = text_corpus_clean_4,
                              FUN = stemDocument)

#remove whitespace
text_corpus_clean_6 <- tm_map(x = text_corpus_clean_5,
                              FUN = stemDocument)
#processing check
text_corpus_clean_1[[1]] %>% 
  as.character()
text_corpus_clean_2[[1]] %>% 
  as.character()
text_corpus_clean_3[[1]] %>% 
  as.character()
text_corpus_clean_4[[1]] %>% 
  as.character()
text_corpus_clean_5[[1]] %>% 
  as.character()
text_corpus_clean_6[[1]] %>% 
  as.character()


#tokenization
text_DTM <- DocumentTermMatrix(text_corpus_clean_6)
inspect(text_DTM)

#Classify into training and testing data
train_index <- sample(x = 1:n, size = round(n*0.7), replace = FALSE)

DTM_train <- text_DTM[train_index,]
DTM_test <- text_DTM[-train_index,]

labels_train <- data_1$type[train_index]
labels_test <- data_1$type[-train_index]

table(labels_train) %>% 
  prop.table()
table(labels_test) %>% 
  prop.table()

#wordclous - text total
library(wordcloud)
windows()
wordcloud(words = text_corpus_clean_6, 
          min.freq = 50, 
          random.order = FALSE, scale = c(5,0.5))

index_ham <- which(data_1$type == "ham")
index_spam <- which(data_1$type == "spam")

#wordclous - text ham and text spam
windows()
wordcloud(words = text_corpus_clean_6[index_ham], 
          min.freq = 50, 
          random.order = FALSE, scale = c(5,0.5))
windows()
wordcloud(words = text_corpus_clean_6[index_spam], 
          min.freq = 10, 
          random.order = FALSE, scale = c(5,0.5))

#Frequnetly words
index_frequnet_words <- findFreqTerms(x = DTM_train, lowfreq = 5) 
DTM_train_freq <- DTM_train[,index_frequnet_words]
DTM_test_freq <- DTM_test[,index_frequnet_words]


#categorize function
fn_convert_counts <- function(x) {
  value <- ifelse(x > 0, "Yes", "No")
  return(value)
}

data_train_freq <- apply(X = DTM_train_freq, MARGIN = 2, FUN = fn_convert_counts) 
data_test_freq <- apply(X = DTM_test_freq, MARGIN = 2, FUN = fn_convert_counts) 
View(data_train_freq)



## Step.3 Train model 
library(naivebayes)
sms_classifier <- naive_bayes(x = data_train_freq, y = labels_train)



## Step.4 Evaluate model
pred_labels_test <- predict(object = sms_classifier, data_test_freq)

library(gmodels)
CrossTable(x = labels_test, y = pred_labels_test)



## Step.5 Improve model
sms_classifier_2 <- naive_bayes(x = data_train_freq, y = labels_train, laplace = 1)
pred_labels_test_2 <- predict(object = sms_classifier_2, data_test_freq)
CrossTable(x = labels_test, y = pred_labels_test_2)
