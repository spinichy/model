###Part 1


##Step1. Data collection


library(tidyverse)
data_raw <- read.table(file = "letter-recognition.txt", sep = ",")
str(data_raw)

letter <- data_raw


##Step2. Data preparation and exploration


library(mice)
md.pattern(letter) #No NA value

summary(letter)

colnames(letter) <- c("class", "xbox","ybox","width","high","onpix",
                      "x-bar","y-bar","x2ba","y2bar","xybar","x2ybr",
                      "xy2b"," x-ege","xegvy","y-ege","yegvx")
colnames(letter) %>% 
  unique() %>% 
  length() #No duplicated colnames

letter$class <- as.factor(letter$class) #factor(all information should numeric)

str(letter)
#split into train and test data

index_train <- sample(x = 1:nrow(letter), 
                      size = nrow(letter)*0.8, 
                      replace = FALSE)
letter_train <- letter[index_train ,]
letter_test <- letter[-index_train ,]


##Step3.Train Model


library(kernlab)
model_letter_linear <- ksvm(class ~ ., 
                            data = letter_train, 
                            kernel = "vanilladot",#Linear kernel
                            C = 1) #Cost 1


##Step4.Evaluate Model


pred_letter <- predict(object = model_letter_linear, 
                       newdata = letter_test)

table(pred_letter, letter_test$class)
table(pred_letter == letter_test$class) %>% 
  prop.table()


##Step5.Improve Model


set.seed(1234) #Cross-validation, SMO. casue to find sigma of gaussian
model_letter_gaussian <- ksvm(class ~ ., 
                       data = letter_train, 
                       kernel = "rbfdot",#gaussian kernel
                       C = 1) #Cost 1

pred_letter_gaussian <- predict(object = model_letter_gaussian, 
                                newdata = letter_test)

table(pred_letter_gaussian, letter_test$class)
table(pred_letter_gaussian == letter_test$class) %>% 
  prop.table() 



cost_values <- c(1,seq(5,50,by=5));cost_values

accuracy <- c()
j <- 1
for(i in cost_values) {
  set.seed(1234)
  model_letter_gaussian <- ksvm(class ~ ., 
                                data = letter_train, 
                                kernel = "rbfdot",
                                C = i)
  
  pred_letter_gaussian <- predict(object = model_letter_gaussian, 
                                  newdata = letter_test)
  
  accuracy[j] <- mean(pred_letter_gaussian == letter_test$class)
  j <- j + 1
}

plot(x = cost_values, y = accuracy, 
     main = "Character prediction accuracy using SVM", 
     pch = 1, type = "b",
     ylim = c(0.92,1.0), xaxt = "n")
axis(side = 1, at = cost_values)
text(x = cost_values, y = accuracy, 
     labels = paste0(round(accuracy*100,2),"%"), pos = 3)

index_max <- which.max(accuracy)
points(x = cost_values[index_max], 
       y = accuracy[index_max], 
       pch = 19, col = "red")
grid()




###Part 2




##Step1. Data collection
library(tidyverse)
data(iris)
str(iris)


##Step2. Data preparation and exploration


library(mice)
md.pattern(iris) #No NA value

iris_sub <- iris %>% 
  select("Sepal.Length","Sepal.Width", "Species") %>% 
  subset(Species %in% c("setosa","versicolor"))

str(iris_sub)
         
library(ggplot2)        
ggplot(data = iris_sub, 
       mapping = aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point(aes(color = Species))


#split into train and test data
index_train <- sample(x = 1:nrow(iris_sub), 
                      size = nrow(iris_sub)*0.8, 
                      replace = FALSE)

train_iris_sub <- iris_sub[index_train,]
test_iris_sub <- iris_sub[-index_train,]


##Step3.Train Model


library(e1071)
set.seed(1234)
model_iris_sub <- svm(Species ~ ., 
                      data = iris_sub, 
                      kernel = "linear", 
                      cost = 1, 
                      scale = FALSE) #standardization



summary(model_iris_sub)
index_support.vector <- model_iris_sub$index 

alpha <- abs( t(model_iris_sub$coefs) )
ys <- sign( t(model_iris_sub$coefs) )
xs <- model_iris_sub$SV #x: support vectors

w <- (alpha*ys) %*% xs;w
bias <- t(ys) - (xs %*% t(w))
mean(b) # 4.984947
b <- model_iris_sub$rho #-4.941785


ggplot(data = iris_sub, 
       mapping = aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point(aes(color = Species)) + 
  geom_point(data = iris_sub[index_support.vector,-3], shape = 21, size = 5) +
  geom_abline(intercept = b/w[1,2], 
              slope = -(w[1,1]/w[1,2]),  #middle boundary w1x1 + w2x2 + b = 0
              lty = "dashed", lwd = 1)


##Step4.Evaluate Model


pred_iris_sub <- predict(object = model_iris_sub, 
                         newdata = test_iris_sub )

table(test_iris_sub$Species, pred_iris_sub, 
      dnn = c("actual", "predict"))



##Step5.Improve Model


set.seed(1234)
model_iris_sub_2 <- svm(Species ~ ., 
                      data = iris_sub, 
                      kernel = "linear", 
                      cost = 100, 
                      scale = FALSE)

pred_iris_sub_2 <- predict(object = model_iris_sub_2, 
                         newdata = test_iris_sub )

table(test_iris_sub$Species, pred_iris_sub_2, 
      dnn = c("actual", "predict"))




###part3




##Step1. Data collection
library(tidyverse)
library(AER)
data("Affairs")
str(Affairs)


##Step2. Data preparation and exploration


affair <- Affairs

library(mice)
md.pattern(affair) #No NA value

affair$affairs <- factor(x = ifelse(affair$affairs > 0, 1, 0), 
                         levels = c(1,0), 
                         labels = c("Yes","No"))
str(affair)


#split into train and test data
index_train <- sample(x = 1:nrow(affair), 
                      size = nrow(affair)*0.7, 
                      replace = FALSE)

train_affair <- affair[index_train,]
test_affair <- affair[-index_train,]

table(train_affair$affairs)
table(test_affair$affairs)

##Step3.Train Model


library(e1071)
set.seed(1234)
model_affair <- svm(affairs ~ ., 
                    data = train_affair,
                    kernel = "radial", 
                    cost = 1,
                    scale = TRUE) 

summary(model_affair)

#multidimensional scaling
library(cluster)
library(ggplot2)
dist <- daisy(train_affair[,-1]) #distance matrix., if category, same = 0, not same = 1
affair_mds <- cmdscale(d = dist, k = 2) %>% #eigen
  data.frame()

index_support.vector <- model_affair$index 

ggplot(data = affair_mds , 
       mapping = aes(x = X1, y = X2)) + 
  geom_point(aes(color = train_affair[,1], 
                 shape = train_affair[,1])) + 
  geom_point(data = affair_mds[index_support.vector,], shape = 21, size = 5) +
  labs(color = "affair",
       shape = "affair")


##Step4.Evaluate Model


pred_affair <- predict(object = model_affair , 
                       newdata = test_affair)

table(test_affair$affairs, pred_affair, 
      dnn = c("actual", "predict"))

mean(test_affair$affairs == pred_affair) #0.8066298


##Step5.Improve Model


set.seed(1234)
model_affair_2 <- tune.svm(affairs ~ ., 
                     data = train_affair,
                     kernel = "radial", 
                     cost = 2^(-5:5),
                     gamma = 10^(-3:3),
                     scale = TRUE) 

summary(model_affair_2)
model_affair_2$best.parameters

model_affair_3 <- svm(affairs ~ ., 
                      data = train_affair,
                      kernel = "radial", 
                      cost = 2,
                      gamma = 0.1,
                      scale = TRUE) 


pred_affair_2 <- predict(object = model_affair_3, 
                         newdata = test_affair )

table(test_affair$affairs, pred_affair_2, 
      dnn = c("actual", "predict"))

mean(test_affair$affairs == pred_affair_2) #0.8066298
